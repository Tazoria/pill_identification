{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 필요한 모듈들 임포트"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine tuning\n",
    "사전 훈련된 모델을 기반으로 아키텍쳐를 새로운 목적에 맞게 변형하고 이미 학습된 모델의 가중치를 미세하게 조정해 학습시키는 방법\n",
    "    - 사전 훈련된 모델의 가중치를 초기값으로 사용하고 추가로 학습\n",
    "    - CNN 베이스의 사전학습 모델을 사용할 때에는 이전에 학습한 내용들을 모두 잊어버릴 위험이 있기 때문에 작은 learning rate를 사용하는것이 바람직함\n",
    "[방법1] 모델 전체를 새로 학습\n",
    "- 사전 학습 모델의 구조만 사용하면서, 자신의 데이터셋에 맞게 모델을 전부 새롭게 학습시키는 방법\n",
    "- 데이터의 크기가 크고 유사성이 작을 때 사용\n",
    "[방법2] Conv base는 일부분 고정(Freezing)하고 나머지 Conv Base 계층과 Classifier를 새롭게 학습\n",
    "- Conv base: 합성곱 층과 풀링 층이 여러 겹 쌓여있는 부분으로 특징을 추출하는 역할\n",
    "- Classifier: 주로 완전연결계층으로 구성되며 Conv base가 추출한 특징들을 잘 학습해 각각의 샘플들을 알맞은 class로 분류\n",
    "- 낮은 레벨의 계층은 일반적이고 독리적인 특징(신형성)을 추출하고, 높은 레벨의 계층은 구체적이고 명확한 특징(형상)을 추출\n",
    "- 크기가  크고 유사성도 높은 데이터셋일 때\n",
    "[방법3] Conv Base는 고정하고 Classifier만 새로 학습\n",
    "- 컴퓨팅 연산 능력이 부족하거나, 데이터셋이 너무 작을 때, 혹은 적용하려는 task와 사전학습에 쓰인 데이터가 매우 비슷할 때 사용\n",
    "\n",
    "[크기가 작고 유사성도 작은 데이터]\n",
    "- 방법2를 쓰되 조금 더 깊은 계층까지 새로 학습시키\n",
    "- Data Augmentation 하기\n",
    "\n",
    "[Feature Extraction]\n",
    "사전 훈련된 모델의 하위 층을 동결하고, 상위 층을 새로운 작업을 위해 수정하지 않고 사용하는 것 -> 데이터분류기(마지막 완전연결층) 부분만 새로 만드는 것\n",
    "    - 사전 훈련된 모델(고정): 중요한 특성 추출(학습 X)\n",
    "    - 데이터분류기: 추출된 특성을 입력받아 최종적으로 이미지에 대한 클래스를 분류(학습O)\n",
    "    - 가능한 모델: Xception, Inception V3, ResNet50, VGG16, VGG19, MobileNet\n",
    "\n",
    "[ 분류기 수정하는 방법 ]\n",
    "Fine tunning을 하기 전 input값으로 받는 이미지의 크기는 무조건 확인해야함\n",
    "    - 대부분의 모델은 (224, 224)이지만 inception_v3의 경우 (299, 299)\n",
    "1. print(model)로 마지막 층의 in_features 확인\n",
    "2. nn.Linear(in_features, num_classes)로 클래스 수 변경\n",
    "    - 예시1: Resnet\n",
    "        - (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
    "            - => model.fc = nn.Linear(512, num_classes)\n",
    "    - 예시2: Alexnet\n",
    "        - (classifier): Sequential(... (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
    "            - => model.classifier[6] = nn.Linear(4096, num_classes)\n",
    "    - 예시3(특이구조): Squeezenet\n",
    "        - - output은 classifier의 첫번째 레이어인 1X1 convolution layer로부터 나옴\n",
    "        - (classifier): Sequential((0): Dropout(p=0.5)  (1): Conv2d(512, 1000, kernel_size=13, stride=1, padding=0)  (2): ReLU(inplace)  (3): AvgPool2d(kernel_size=13, stride=1, padding=0))\n",
    "            -  => model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "    - 예시4(특이구조): Inception v3\n",
    "        - Rethinking을 하기 때문에 학습과정에서 output이 두개의 레이어로부터 나옴\n",
    "        - 두번째 output(auxiliary output): AuxLogits 부분을 포함하는 네트워크에 포함됨\n",
    "        - 주된 output은 네트워크의 마지막 레이어에서 출력됨\n",
    "            -  test 시에는 이 output만 고려함\n",
    "        - (AuxLogits): InceptionAux(... (fc): Linear(in_feature=768, out_features=1000, bias=True) ... (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
    "            -  finetune 하기 위해서는 두 레이어를 reshape 해줘야함\n",
    "            -  => model.AuxLogits.fc = nn.Linear(768, num_classes)\n",
    "            -  model.fc = nn.Linear(2048, nuum_classes)\n",
    "\n",
    "[ 사용가능한 함수들 ]\n",
    "- get_model(name, **config): 모델 이름과 환경설정을 인수로 받아 해당 모델의 인스턴스를 반환\n",
    "    - get_model(\"quantized_mobilenet_v3_large\", weights=\"DEFAULT\")\n",
    "- get_model_weight(name): 해당 모델의 열거형 가중치 클래스 반환\n",
    "    - 예1: get_model_weights(\"quantized_mobilenet_v3_large\")\n",
    "    - 예2: get_model_weights(torchvision.models.quantization.mobilenet_v3_large)\n",
    "    - 예1 = 예2\n",
    "- get_weight(name): 열거형 가중치 변수의 이름으로 값을 가져옴\n",
    "    - 예: get_weight(\"MobileNet_V3_Large_QuantizedWeights.DEFAULT\")\n",
    "- list_models(\\[module]): 해당 이름으로 등록된 모델 목록을 반환\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 사용가능한 모델 확인\n",
    "for name in dir(torchvision.models):\n",
    "    print(name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 데이터셋 만들기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 데이터 로드 및 전처리\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# CIFAR-10 데이터셋 로드\n",
    "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 구성"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "\"\"\"\n",
    "        MobileNet V3 main class\n",
    "\n",
    "        Args:\n",
    "            inverted_residual_setting (List[InvertedResidualConfig]): Network structure\n",
    "            last_channel (int): The number of channels on the penultimate layer\n",
    "            num_classes (int): Number of classes\n",
    "            block (Optional[Callable[..., nn.Module]]): Module specifying inverted residual building block for mobilenet\n",
    "            norm_layer (Optional[Callable[..., nn.Module]]): Module specifying the normalization layer to use\n",
    "            dropout (float): The droupout probability\n",
    "        \"\"\"\n",
    "# MobileNetV3 Large 모델 직접 구현\n",
    "class CustomMobileNetV3Large(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(CustomMobileNetV3Large, self).__init__()\n",
    "        self.features = mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.DEFAULT, progress=True).features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(960, num_classes, 1, 1, 0),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 학습"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "model = CustomMobileNetV3Large(num_classes=100).to(device)\n",
    "model.train()  # 모델을 학습 모드로 설정\n",
    "\n",
    "# Fine-tuning을 위한 손실함수 및 옵티마이저  설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Fine-tuning을 위한 반복 학습\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "      inputs, labels = inputs.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 추론 - 테스트 데이터셋"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 테스트 데이터셋\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 추론 - 임의의 이미지"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img = cv2.imread('apple.jpg')\n",
    "img = transform_test(img).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(img.to(device))\n",
    "\n",
    "_, predicted_class = torch.max(outputs, 1)\n",
    "predicted_class = predicted_class.item()\n",
    "predicted_class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 작업중"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 기본설정"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_list = ['resnet', 'alexnet', 'vgg', 'squeezenet', 'densenet', 'inception', 'mobilenet']\n",
    "# model list로부터 사용할 모델 고르기\n",
    "model_name = 'mobilenet'\n",
    "\n",
    "# 데이터셋에 있는 클래스 수\n",
    "num_classes = 100\n",
    "# 학습 배치사이즈\n",
    "batch_size = 64\n",
    "# 학습 에폭\n",
    "num_epochs = 5\n",
    "# 피쳐 추출을 위한 Flag\n",
    "#   False: 모델 전체를 파인튜닝\n",
    "#   True: 매개변수만 업데이트\n",
    "feature_extract = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 모델학습 헬퍼 함수\n",
    "def train_models(model, dataloaders, criterion, optimizer, num_epochs=10, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch} / {num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # 각 에폭은 학습/검증 phase를 가짐\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 트레이닝 모드 설정\n",
    "            else:\n",
    "                model.eval()  # 추론 모드 설정\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # 데이터 학습하기\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # 파라미터 기울기 초기화\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # 학습 모드인 경우에만 history 추적\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # 모델의 ouptut과 loss를 구함\n",
    "                    # inception의 경우 학습시 auxiliary output이 있는 특수 케이스임.\n",
    "                    #   학습시: final output과 auxiliary output을 더하는 과정이 필요함\n",
    "                    #   테스트시: final output만 고려\n",
    "                    # Auxilary output을 같이 고려해야하는 학습단계\n",
    "                    if is_inception and phase == 'train':\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4 * loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # 학습(backward + optimize)\n",
    "                    if phase == 'train':\n",
    "                        loss.backwawrd()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # loss 구하기\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # 모델 깊은복사\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.4f}'.format(best_acc))\n",
    "\n",
    "    # 베스트 모델 가중치를 로드\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 모델 파라미터의 requires_grad 속성\n",
    "- 기본적으로 사전  훈련된 모델을 로드할 때 모든 매개변수의 requires_grad = True로 설정되어 있음\n",
    "    - True: 처음부터 훈련하거나 fine tuning할 때는 True\n",
    "    - False(Freeze): 기존 layer의 weight를 고정"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 초기화"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True, quantize=False):\n",
    "    # 모델마다 다르게 지정될 변수들 초기화\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == 'resnet':\n",
    "        if quantize:\n",
    "            '''Quantized Resnet50'''\n",
    "            weights = models.quantization.ResNet50_QuantizedWeights.DEFAULT\n",
    "            model_ft = models.quantization.resnet50(weights=weights, quantize=True)\n",
    "        else:\n",
    "            '''Resnet50'''\n",
    "            model_ft = models.resnet50(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == 'alexnet':\n",
    "        '''AlexNet'''\n",
    "        model_ft = models.alexnet(pretrianed=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_featrues\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == 'vgg':\n",
    "        '''VGG11_bn'''\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == 'squeezenet':\n",
    "        '''Squeezenet 1.0'''\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == 'densenet':\n",
    "        '''Densenet 121'''\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == 'inception':\n",
    "        '''Inception V3'''\n",
    "        if quantize:\n",
    "            weights = models.quantization.Inception_V3_QuantizedWeights.DEFAULT\n",
    "            model_ft = models.quantization.inception_v3(weights=weights, quantize=True)\n",
    "        else:\n",
    "            model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 299  # 다른 모델과 다르게 299 사이즈를 사용\n",
    "    else:\n",
    "        print('모델의 이름을 잘못 입력하여 종료합니다...')\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 모델 인스턴스 생성하고 구조 확인하기\n",
    "# 1. 일반 레즈넷, 마지막 fc레이어만 변경\n",
    "model_name = 'resnet'\n",
    "num_classes = 100\n",
    "feature_extract = False\n",
    "use_pretrained = True\n",
    "quantize = False\n",
    "\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True, quantize=False)\n",
    "print(model_name)\n",
    "print(model_ft)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. 양자화된 레즈넷, 마지막 fc레이어만 변경 - QuantizedLinear 나와야하는데 일반 Linear만나옴\n",
    "model_name = 'resnet'\n",
    "num_classes = 100\n",
    "feature_extract = False\n",
    "use_pretrained = True\n",
    "quantize = True\n",
    "\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True, quantize=True)\n",
    "print('quantized ' + model_name)\n",
    "print(model_ft)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 헬퍼함수 없이 만든 양자화된 레즈넷\n",
    "weights = models.quantization.ResNet50_QuantizedWeights.DEFAULT\n",
    "model_ft = models.quantization.resnet50(weights=weights, quantize=True)\n",
    "print(model_ft)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/81 [00:00<00:55,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_name': [1, 2], 'dir': [0, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/81 [00:01<00:51,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_name': [1, 2], 'dir': [0, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 3/81 [00:01<00:46,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_name': [1, 2], 'dir': [0, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4/81 [00:02<00:44,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_name': [1, 2], 'dir': [0, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5/81 [00:02<00:42,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_name': [1, 2], 'dir': [0, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5/81 [00:03<00:53,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_name': [1, 2], 'dir': [0, 1]}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m source_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../../data/training/labels\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 3\u001B[0m json_dict \u001B[38;5;241m=\u001B[39m \u001B[43mextraction\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(json_dict)\n",
      "Cell \u001B[1;32mIn[17], line 32\u001B[0m, in \u001B[0;36mextraction\u001B[1;34m(source_path, format)\u001B[0m\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m zipfile\u001B[38;5;241m.\u001B[39mZipFile(zip_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m zip_ref:\n\u001B[0;32m     31\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m---> 32\u001B[0m             json_dict \u001B[38;5;241m=\u001B[39m \u001B[43mget_json\u001B[49m\u001B[43m(\u001B[49m\u001B[43mextract_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mzip_ref\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m json_dict\n",
      "Cell \u001B[1;32mIn[17], line 41\u001B[0m, in \u001B[0;36mget_json\u001B[1;34m(extract_path, zip_ref)\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_json\u001B[39m(extract_path, zip_ref):\n\u001B[0;32m     39\u001B[0m     json_dict \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfile_name\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m],\n\u001B[0;32m     40\u001B[0m                  \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdir\u001B[39m\u001B[38;5;124m'\u001B[39m : [\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m]}\n\u001B[1;32m---> 41\u001B[0m     \u001B[38;5;28;43mprint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mjson_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     42\u001B[0m     \u001B[38;5;66;03m# zip_file_contents = zip_ref.namelist()\u001B[39;00m\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;66;03m# # for item in zip_file_contents:\u001B[39;00m\n\u001B[0;32m     44\u001B[0m     \u001B[38;5;66;03m# #     if item.endswith('.json'):\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     49\u001B[0m     \u001B[38;5;66;03m#     json_dict['file_name'].append(file_name)\u001B[39;00m\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;66;03m#     json_dict['dir'].append(item)\u001B[39;00m\n\u001B[0;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m json_dict\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\iostream.py:648\u001B[0m, in \u001B[0;36mOutStream.write\u001B[1;34m(self, string)\u001B[0m\n\u001B[0;32m    646\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpub_thread\u001B[38;5;241m.\u001B[39mschedule(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flush)\n\u001B[0;32m    647\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 648\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_schedule_flush\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    650\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(string)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\iostream.py:545\u001B[0m, in \u001B[0;36mOutStream._schedule_flush\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    542\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_schedule_in_thread\u001B[39m():\n\u001B[0;32m    543\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_io_loop\u001B[38;5;241m.\u001B[39mcall_later(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mflush_interval, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flush)\n\u001B[1;32m--> 545\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpub_thread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mschedule\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_schedule_in_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\iostream.py:251\u001B[0m, in \u001B[0;36mIOPubThread.schedule\u001B[1;34m(self, f)\u001B[0m\n\u001B[0;32m    249\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_events\u001B[38;5;241m.\u001B[39mappend(f)\n\u001B[0;32m    250\u001B[0m     \u001B[38;5;66;03m# wake event thread (message content is ignored)\u001B[39;00m\n\u001B[1;32m--> 251\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_event_pipe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    252\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    253\u001B[0m     f()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\zmq\\sugar\\socket.py:696\u001B[0m, in \u001B[0;36mSocket.send\u001B[1;34m(self, data, flags, copy, track, routing_id, group)\u001B[0m\n\u001B[0;32m    689\u001B[0m         data \u001B[38;5;241m=\u001B[39m zmq\u001B[38;5;241m.\u001B[39mFrame(\n\u001B[0;32m    690\u001B[0m             data,\n\u001B[0;32m    691\u001B[0m             track\u001B[38;5;241m=\u001B[39mtrack,\n\u001B[0;32m    692\u001B[0m             copy\u001B[38;5;241m=\u001B[39mcopy \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    693\u001B[0m             copy_threshold\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy_threshold,\n\u001B[0;32m    694\u001B[0m         )\n\u001B[0;32m    695\u001B[0m     data\u001B[38;5;241m.\u001B[39mgroup \u001B[38;5;241m=\u001B[39m group\n\u001B[1;32m--> 696\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mflags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrack\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrack\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mzmq\\backend\\cython\\socket.pyx:742\u001B[0m, in \u001B[0;36mzmq.backend.cython.socket.Socket.send\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mzmq\\backend\\cython\\socket.pyx:789\u001B[0m, in \u001B[0;36mzmq.backend.cython.socket.Socket.send\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mzmq\\backend\\cython\\socket.pyx:250\u001B[0m, in \u001B[0;36mzmq.backend.cython.socket._send_copy\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\zmq\\backend\\cython\\checkrc.pxd:13\u001B[0m, in \u001B[0;36mzmq.backend.cython.checkrc._check_rc\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T12:03:10.919602700Z",
     "start_time": "2023-09-21T12:03:07.073041700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
