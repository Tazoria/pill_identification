{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 필요한 모듈들 임포트"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine tuning\n",
    "사전 훈련된 모델을 기반으로 아키텍쳐를 새로운 목적에 맞게 변형하고 이미 학습된 모델의 가중치를 미세하게 조정해 학습시키는 방법\n",
    "    - 사전 훈련된 모델의 가중치를 초기값으로 사용하고 추가로 학습\n",
    "    - CNN 베이스의 사전학습 모델을 사용할 때에는 이전에 학습한 내용들을 모두 잊어버릴 위험이 있기 때문에 작은 learning rate를 사용하는것이 바람직함\n",
    "[방법1] 모델 전체를 새로 학습\n",
    "- 사전 학습 모델의 구조만 사용하면서, 자신의 데이터셋에 맞게 모델을 전부 새롭게 학습시키는 방법\n",
    "- 데이터의 크기가 크고 유사성이 작을 때 사용\n",
    "[방법2] Conv base는 일부분 고정(Freezing)하고 나머지 Conv Base 계층과 Classifier를 새롭게 학습\n",
    "- Conv base: 합성곱 층과 풀링 층이 여러 겹 쌓여있는 부분으로 특징을 추출하는 역할\n",
    "- Classifier: 주로 완전연결계층으로 구성되며 Conv base가 추출한 특징들을 잘 학습해 각각의 샘플들을 알맞은 class로 분류\n",
    "- 낮은 레벨의 계층은 일반적이고 독리적인 특징(신형성)을 추출하고, 높은 레벨의 계층은 구체적이고 명확한 특징(형상)을 추출\n",
    "- 크기가  크고 유사성도 높은 데이터셋일 때\n",
    "[방법3] Conv Base는 고정하고 Classifier만 새로 학습\n",
    "- 컴퓨팅 연산 능력이 부족하거나, 데이터셋이 너무 작을 때, 혹은 적용하려는 task와 사전학습에 쓰인 데이터가 매우 비슷할 때 사용\n",
    "\n",
    "[크기가 작고 유사성도 작은 데이터]\n",
    "- 방법2를 쓰되 조금 더 깊은 계층까지 새로 학습시키\n",
    "- Data Augmentation 하기\n",
    "\n",
    "[Feature Extraction]\n",
    "사전 훈련된 모델의 하위 층을 동결하고, 상위 층을 새로운 작업을 위해 수정하지 않고 사용하는 것 -> 데이터분류기(마지막 완전연결층) 부분만 새로 만드는 것\n",
    "    - 사전 훈련된 모델(고정): 중요한 특성 추출(학습 X)\n",
    "    - 데이터분류기: 추출된 특성을 입력받아 최종적으로 이미지에 대한 클래스를 분류(학습O)\n",
    "    - 가능한 모델: Xception, Inception V3, ResNet50, VGG16, VGG19, MobileNet\n",
    "\n",
    "[ 분류기 수정하는 방법 ]\n",
    "Fine tunning을 하기 전 input값으로 받는 이미지의 크기는 무조건 확인해야함\n",
    "    - 대부분의 모델은 (224, 224)이지만 inception_v3의 경우 (299, 299)\n",
    "1. print(model)로 마지막 층의 in_features 확인\n",
    "2. nn.Linear(in_features, num_classes)로 클래스 수 변경\n",
    "    - 예시1: Resnet\n",
    "        - (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
    "            - => model.fc = nn.Linear(512, num_classes)\n",
    "    - 예시2: Alexnet\n",
    "        - (classifier): Sequential(... (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
    "            - => model.classifier[6] = nn.Linear(4096, num_classes)\n",
    "    - 예시3(특이구조): Squeezenet\n",
    "        - - output은 classifier의 첫번째 레이어인 1X1 convolution layer로부터 나옴\n",
    "        - (classifier): Sequential((0): Dropout(p=0.5)  (1): Conv2d(512, 1000, kernel_size=13, stride=1, padding=0)  (2): ReLU(inplace)  (3): AvgPool2d(kernel_size=13, stride=1, padding=0))\n",
    "            -  => model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "    - 예시4(특이구조): Inception v3\n",
    "        - Rethinking을 하기 때문에 학습과정에서 output이 두개의 레이어로부터 나옴\n",
    "        - 두번째 output(auxiliary output): AuxLogits 부분을 포함하는 네트워크에 포함됨\n",
    "        - 주된 output은 네트워크의 마지막 레이어에서 출력됨\n",
    "            -  test 시에는 이 output만 고려함\n",
    "        - (AuxLogits): InceptionAux(... (fc): Linear(in_feature=768, out_features=1000, bias=True) ... (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
    "            -  finetune 하기 위해서는 두 레이어를 reshape 해줘야함\n",
    "            -  => model.AuxLogits.fc = nn.Linear(768, num_classes)\n",
    "            -  model.fc = nn.Linear(2048, nuum_classes)\n",
    "\n",
    "[ 사용가능한 함수들 ]\n",
    "- get_model(name, **config): 모델 이름과 환경설정을 인수로 받아 해당 모델의 인스턴스를 반환\n",
    "    - get_model(\"quantized_mobilenet_v3_large\", weights=\"DEFAULT\")\n",
    "- get_model_weight(name): 해당 모델의 열거형 가중치 클래스 반환\n",
    "    - 예1: get_model_weights(\"quantized_mobilenet_v3_large\")\n",
    "    - 예2: get_model_weights(torchvision.models.quantization.mobilenet_v3_large)\n",
    "    - 예1 = 예2\n",
    "- get_weight(name): 열거형 가중치 변수의 이름으로 값을 가져옴\n",
    "    - 예: get_weight(\"MobileNet_V3_Large_QuantizedWeights.DEFAULT\")\n",
    "- list_models(\\[module]): 해당 이름으로 등록된 모델 목록을 반환\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  2.0.1+cu117\n",
      "Torchvision Version:  0.15.2+cpu\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "import pandas\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from thop import profile\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "from torch.utils.data import random_split, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from custom_models import CustomMobileNetV3Large, QuantizedCustomMobileNetV3Large, CustomMnasNet1_3, QuantizedCustomMnasNet1_3\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T11:32:52.279927700Z",
     "start_time": "2023-10-15T11:32:48.341978300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "model_list = ['resnet', 'alexnet', 'vgg', 'squeezenet', 'densenet', 'inception', 'mobilenet']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 모델 파라미터의 requires_grad 속성\n",
    "- 기본적으로 사전  훈련된 모델을 로드할 때 모든 매개변수의 requires_grad = True로 설정되어 있음\n",
    "    - True: 처음부터 훈련하거나 fine tuning할 때는 True\n",
    "    - False(Freeze): 기존 layer의 weight를 고정"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 모델학습 헬퍼 함수\n",
    "def train_models(model, dataloaders, criterion, optimizer, num_epochs=5, is_inception=False, quantize=False, memo=None):\n",
    "    torch.cuda.empty_cache()\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    val_top5_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    patience = 3\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1} / {num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # 각 에폭은 학습/검증 phase를 가짐\n",
    "        for phase in ['train', 'val']:\n",
    "            print(f'>>>>> phase: {phase}<<<<<')\n",
    "            if phase == 'train':\n",
    "                model.train()  # 트레이닝 모드 설정\n",
    "            else:\n",
    "                if quantize == True:\n",
    "                    # 모델을 양자화 + 추론모드\n",
    "                    model = torch.quantization.convert(model.eval(), inplace=False)\n",
    "                else:\n",
    "                    model.eval()  # 추론 모드 설정\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            running_top5_corrects = 0\n",
    "\n",
    "            # 데이터 학습하기\n",
    "            for inputs, labels in tqdm_notebook(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # 파라미터 기울기 초기화\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # 학습 모드인 경우에만 history 추적\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # 모델의 ouptut과 loss를 구함\n",
    "                    # inception의 경우 학습시 auxiliary output이 있는 특수 케이스임.\n",
    "                    #   학습시: final output과 auxiliary output을 더하는 과정이 필요함\n",
    "                    #   테스트시: final output만 고려\n",
    "                    # Auxilary output을 같이 고려해야하는 학습단계\n",
    "                    if is_inception and phase == 'train':\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4 * loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # top-5 정확도 계산\n",
    "                    _,top5_preds = torch.topk(outputs, 5)\n",
    "                    top5_corrects = torch.sum(top5_preds == labels.view(-1, 1))\n",
    "\n",
    "                    # 학습(backward + optimize)\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        if (epoch + 1) % 5 == 0:\n",
    "                            # 모델 및 다른 정보 저장\n",
    "                            save_path = f'save/[{type(model).__name__}]{memo}_epoch{epoch+1}).pth'\n",
    "                            torch.save({\n",
    "                                'model_state_dict': model.state_dict(),  # 모델 가중치 저장\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),  # 옵티마이저 상태 저장 (선택적)\n",
    "                                'epoch': epoch+1,  # 현재 학습 에폭 저장 (선택적)\n",
    "                                # 다른 필요한 정보 저장 (선택적)\n",
    "                            }, save_path)\n",
    "\n",
    "                # loss 구하기\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                running_top5_corrects += top5_corrects\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            epoch_top5_acc = running_top5_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # 모델 깊은복사\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                no_improvement_count = 0  # 개선이 있었으므로 카운트 초기화\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc.item())\n",
    "                val_top5_acc_history.append(epoch_top5_acc.item())\n",
    "                no_improvement_count += 1  # 개선이 없었으므로 카운트\n",
    "\n",
    "            # Early Stopping 확인\n",
    "            # if no_improvement_count >= patience:\n",
    "            #     print(f\"No improvement in validation accuracy for {patience} epochs. Early stopping...\")\n",
    "            #     break  # 학습 종료\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.4f}'.format(best_acc))\n",
    "    print('Best top5 val Acc: {:.4f}'.format(epoch_top5_acc))\n",
    "\n",
    "    # 베스트 모델 가중치를 로드\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history, val_top5_acc_history\n",
    "\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T11:32:52.324776800Z",
     "start_time": "2023-10-15T11:32:52.293860300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=True, quantize=False):\n",
    "    # 모델마다 다르게 지정될 변수들 초기화\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == 'resnet':\n",
    "        if quantize:\n",
    "            '''Quantized Resnet50'''\n",
    "            weights = models.quantization.ResNet50_QuantizedWeights.DEFAULT\n",
    "            model_ft = models.quantization.resnet50(weights=weights, quantize=True)\n",
    "        else:\n",
    "            '''Resnet50'''\n",
    "            model_ft = models.resnet50(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == 'mobilenet':\n",
    "        if quantize:\n",
    "            '''Quantized Mobilenet v3 large'''\n",
    "            model_ft = QuantizedCustomMobileNetV3Large(num_classes=500).to('cpu')\n",
    "\n",
    "            # 양자화 설정 - gbgemm 백엔드 사용\n",
    "            backend = 'fbgemm'\n",
    "            # 양자화 스키마 설정 (예: symmetric)\n",
    "            quantization_params = torch.quantization.get_default_qconfig(backend)\n",
    "            quantization_params = torch.quantization.QConfig(activation=quantization_params.activation, weight=quantization_params.weight)\n",
    "\n",
    "            # 양자화 준비\n",
    "            model_ft.qconfig = quantization_params\n",
    "            model_ft= torch.quantization.prepare_qat(model_ft, inplace=False)\n",
    "\n",
    "        else:\n",
    "            model_ft = CustomMobileNetV3Large(num_classes=500).to(device)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == 'mnasnet':\n",
    "        if quantize:\n",
    "            '''Quantized Mnasnet 1_3'''\n",
    "            model_ft = QuantizedCustomMnasNet1_3(num_classes=500).to(device)\n",
    "\n",
    "            # 양자화 설정 - gbgemm 백엔드 사용\n",
    "            backend = 'fbgemm'\n",
    "            model_ft.qconfig = torch.quantization.get_default_qat_qconfig(backend)\n",
    "\n",
    "            # 양자화 준비\n",
    "            model_ft= torch.quantization.prepare_qat(model_ft, inplace=False)\n",
    "\n",
    "        else:\n",
    "            model_ft = models.mnasnet1_3(weights=models.MNASNet1_3_Weights.IMAGENET1K_V1)\n",
    "            num_ftrs = model_ft.classifier[1].in_features\n",
    "            model_ft.classifier[1] = nn.Linear(in_features=1280, out_features=num_classes, bias=True)\n",
    "            # model_ft = CustomMnasNet1_3(num_classes=500).to(device)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == 'efficientnet':\n",
    "        if quantize:\n",
    "            pass\n",
    "        else:\n",
    "            model_ft = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "            num_ftrs = model_ft.classifier[1].in_features\n",
    "            model_ft.classifier[1] = nn.Linear(num_ftrs, num_classes, bias=True)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == 'shufflenet':\n",
    "        if quantize:\n",
    "            pass\n",
    "        else:\n",
    "            model_ft = models.shufflenet_v2_x2_0(weights=models.ShuffleNet_V2_X2_0_Weights.IMAGENET1K_V1)\n",
    "            num_ftrs = model_ft.fc.in_features\n",
    "            model_ft.fc = nn.Linear(num_ftrs, num_classes, bias=True)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == 'alexnet':\n",
    "        '''AlexNet'''\n",
    "        model_ft = models.alexnet(pretrianed=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_featrues\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == 'vgg':\n",
    "        '''VGG11_bn'''\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == 'squeezenet':\n",
    "        '''Squeezenet 1.0'''\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == 'densenet':\n",
    "        '''Densenet 121'''\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == 'inception':\n",
    "        '''Inception V3'''\n",
    "        if quantize:\n",
    "            weights = models.quantization.Inception_V3_QuantizedWeights.DEFAULT\n",
    "            model_ft = models.quantization.inception_v3(weights=weights, quantize=True)\n",
    "        else:\n",
    "            model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 299  # 다른 모델과 다르게 299 사이즈를 사용\n",
    "    else:\n",
    "        print('모델의 이름을 잘못 입력하여 종료합니다...')\n",
    "        exit()\n",
    "\n",
    "    return model_ft.to(device), input_size"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T11:32:52.334750800Z",
     "start_time": "2023-10-15T11:32:52.296852500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RandAugmentation을 적용"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train >  1128 val >  123\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import Grayscale\n",
    "from get_dataloader import get_dataloader\n",
    "\n",
    "# 데이터 변환: 데이터가 부족해 과적합이 발생한 것으로 보이고 색상에 민감해 보여 학습데이터 증강\n",
    "train_dir = r'D:/data/training/sources/train'\n",
    "valid_dir = r'D:/data/training/sources/valid'\n",
    "test_dir = r'D:/data/training/sources/test'\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = get_dataloader(train_dir, batch_size=batch_size, is_train=True, is_grayscale=False)\n",
    "valid_loader = get_dataloader(valid_dir, batch_size=batch_size, is_train=False, is_grayscale=False)\n",
    "test_loader = get_dataloader(test_dir, batch_size=batch_size, is_train=False, is_grayscale=False)\n",
    "\n",
    "dataloaders_dict = {'train': train_loader, 'val': valid_loader}\n",
    "\n",
    "print('train > ', len(train_loader), 'val > ', len(valid_loader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T12:03:55.960200400Z",
     "start_time": "2023-10-15T12:03:41.990413100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 10\n",
      "----------\n",
      ">>>>> phase: train<<<<<\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1128 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0243092e307421cae447c907f6fccd0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5045 Acc: 0.8655\n",
      ">>>>> phase: val<<<<<\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/123 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2deca6c7155649ca81504f1d5034d330"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1616 Acc: 0.9524\n",
      "\n",
      "Epoch 2 / 10\n",
      "----------\n",
      ">>>>> phase: train<<<<<\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1128 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c6c4c6d58f12473e81ebda88823fd42e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1393 Acc: 0.9532\n",
      ">>>>> phase: val<<<<<\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/123 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6a996247c96842ce9541b1ed3d5b2a41"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0859 Acc: 0.9714\n",
      "\n",
      "Epoch 3 / 10\n",
      "----------\n",
      ">>>>> phase: train<<<<<\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1128 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82a7222d2ae04c7985a434a21cc2e56d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1039 Acc: 0.9650\n",
      ">>>>> phase: val<<<<<\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/123 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b1a0239bf6b44faaae6e82a8f1c4b8b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0596 Acc: 0.9770\n",
      "\n",
      "Epoch 4 / 10\n",
      "----------\n",
      ">>>>> phase: train<<<<<\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1128 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "78bfc2f07b654cb7b4f0f0e2af10de0d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0892 Acc: 0.9700\n",
      ">>>>> phase: val<<<<<\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/123 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c52244fb2787462c96227deaf1761310"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0560 Acc: 0.9825\n",
      "\n",
      "Epoch 5 / 10\n",
      "----------\n",
      ">>>>> phase: train<<<<<\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1128 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "31fdd5ab29044822b6cc32f85ce68a0e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0823 Acc: 0.9723\n",
      ">>>>> phase: val<<<<<\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/123 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e5df13f76ef34ddb9879f42e52d9e483"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0400 Acc: 0.9849\n",
      "\n",
      "Epoch 6 / 10\n",
      "----------\n",
      ">>>>> phase: train<<<<<\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1128 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "549abc9362d6422cbcfc5ccc6090eb9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0693 Acc: 0.9768\n",
      ">>>>> phase: val<<<<<\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/123 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "60d7acc6733247989af98f16abfa8610"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0395 Acc: 0.9862\n",
      "\n",
      "Epoch 7 / 10\n",
      "----------\n",
      ">>>>> phase: train<<<<<\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1128 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d18fc3f8845842bd83b5680980148795"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0649 Acc: 0.9784\n",
      ">>>>> phase: val<<<<<\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/123 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38351b3ccfe74a578596ae75df3a71ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0414 Acc: 0.9859\n",
      "\n",
      "Epoch 8 / 10\n",
      "----------\n",
      ">>>>> phase: train<<<<<\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1128 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1fde17f1b1d043239a0e19346af6a877"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0581 Acc: 0.9804\n",
      ">>>>> phase: val<<<<<\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/123 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6aa625d3b5e440ec8bd0627739488f95"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0274 Acc: 0.9910\n",
      "\n",
      "Epoch 9 / 10\n",
      "----------\n",
      ">>>>> phase: train<<<<<\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1128 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9aeb5b9e695148a99355704168447516"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0516 Acc: 0.9827\n",
      ">>>>> phase: val<<<<<\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/123 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da70c37e16244c8886137dac05ee33ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0289 Acc: 0.9909\n",
      "\n",
      "Epoch 10 / 10\n",
      "----------\n",
      ">>>>> phase: train<<<<<\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1128 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "60f4b502589e44ff9c4b6fbfbd3c6d67"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0488 Acc: 0.9839\n",
      ">>>>> phase: val<<<<<\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/123 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3bb47dea20a44a83a7a50363311e66b7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0224 Acc: 0.9921\n",
      "\n",
      "Training complete in 216m 15s\n",
      "Best val Acc: 0.9921\n",
      "Best top5 val Acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "model_name = 'shufflenet'\n",
    "model, _ = initialize_model(model_name='shufflenet', num_classes=500, use_pretrained=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.NAdam(model.parameters(), lr=0.0008)\n",
    "\n",
    "num_epochs = 10\n",
    "model, val_acc_history, val_top5_acc_history = train_models(model, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs, is_inception=False, quantize=False, memo=f'RandAugment({batch_size})_Final')\n",
    "\n",
    "accuracy_mobilenet = {\n",
    "    'top1': val_acc_history,\n",
    "    'top5': val_top5_acc_history}\n",
    "with open(f'save/[Accuracy] {model_name}_RandAugment({batch_size})_Final.json', 'w') as f:\n",
    "    json.dump(accuracy_mobilenet, f)\n",
    "\n",
    "save_path = f'save/[model] {model_name}_RandAugment({batch_size}_Final).pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),  # 모델 가중치 저장\n",
    "    'optimizer_state_dict': optimizer.state_dict(),  # 옵티마이저 상태 저장 (선택적)\n",
    "    'epoch': num_epochs,  # 현재 학습 에폭 저장 (선택적) # 다른 필요한 정보 저장 (선택적)\n",
    "}, save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T15:40:38.560817Z",
     "start_time": "2023-10-15T12:04:23.316570Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 수정 필요\n",
    "def get_graph(accuracy_dict):\n",
    "    plt.title(\"Mobilenet Validation Accuracy: top1 VS top5\")\n",
    "    plt.xlabel(\"Training Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.plot(range(1,num_epochs+1),val_acc_history,label=\"top1 accuracy\", color='purple')\n",
    "    plt.plot(range(1,num_epochs+1),val_top5_acc_history,label=\"top5 accuracy\", color='green')\n",
    "\n",
    "    plt.ylim((0,1.))\n",
    "    plt.yticks([0.8, 0.9, 1.0])\n",
    "    plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train_classes = set()\n",
    "# for data in tqdm_notebook(train_dataset):\n",
    "#     _, label = data  # 데이터와 레이블을 분리\n",
    "#     train_classes.add(label)  # 레이블을 set에 추가하여 중복 클래스 제거\n",
    "#\n",
    "# num_train_classes = len(train_classes)\n",
    "# print(\"훈련 데이터셋 클래스 수:\", num_train_classes)\n",
    "#\n",
    "# valid_classes = set()\n",
    "# for data in tqdm_notebook(valid_dataset):\n",
    "#     _, label = data\n",
    "#     valid_classes.add(label)\n",
    "#\n",
    "# num_valid_classes = len(valid_classes)\n",
    "# print(\"검증 데이터셋 클래스 수:\", num_valid_classes)\n",
    "#\n",
    "# test_classes = set()\n",
    "# for data in tqdm_notebook(test_dataset):\n",
    "#     _, label = data\n",
    "#     test_classes.add(label)\n",
    "#\n",
    "# num_test_classes = len(test_classes)\n",
    "# print(\"테스트 데이터셋 클래스 수:\", num_test_classes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mobilenet, input_size = initialize_model(model_name='mobilenet', num_classes=500, use_pretrained=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.NAdam(mobilenet.parameters(), lr=0.0004)\n",
    "\n",
    "mobilenet, val_acc_history, val_top5_acc_history = train_models(mobilenet, dataloaders_dict, criterion, optimizer, num_epochs=10, is_inception=False, quantize=False)\n",
    "\n",
    "save_path = f'save/mobilenet_epoch10_batch128_pretrained_noQuantize.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': mobilenet.state_dict(),  # 모델 가중치 저장\n",
    "    'optimizer_state_dict': optimizer.state_dict(),  # 옵티마이저 상태 저장 (선택적)\n",
    "    'epoch': 10,  # 현재 학습 에폭 저장 (선택적) # 다른 필요한 정보 저장 (선택적)\n",
    "}, save_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def testing_models(models, dataloaders_dict, num_epochs, num_classes=1000, feature_extract=False, use_pretrained=True, quantize=False):\n",
    "    acc = {}\n",
    "\n",
    "    for model_name in models:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        model, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=use_pretrained, quantize=quantize)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.NAdam(model.parameters(), lr=0.001)\n",
    "\n",
    "        print('device > ', device)\n",
    "        print(f'{model_name} / pretrained({use_pretrained}) / quantize({quantize})')\n",
    "\n",
    "        model, hist, top5_hist = train_models(model, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs, is_inception=False, quantize=quantize)\n",
    "\n",
    "        acc[model_name] = {\n",
    "            'top1': hist,\n",
    "            'top5': top5_hist\n",
    "        }\n",
    "\n",
    "    return accuracies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = 'mobilenet'\n",
    "num_epochs = 2\n",
    "quantize = False\n",
    "model, hist, top5_hist = train_models(model_name, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs, is_inception=False, quantize=quantize)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# models = ['mobilenet', 'mnasnet', 'efficientnet', 'shufflenet']\n",
    "accuracies_quantized = testing_models(models=['mobilenet'], dataloaders_dict=dataloaders_dict, num_epochs=5, num_classes=500, feature_extract=False, use_pretrained=True, quantize=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracies= testing_models(models=['mobilenet'], dataloaders_dict=dataloaders_dict, num_epochs=5, num_classes=500, feature_extract=False, use_pretrained=True, quantize=False)\n",
    "print(accuracies)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('save/accuracies.json', 'w') as f:\n",
    "    json.dump(accuracies, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_path = f'save/mobilenet_epoch10.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': mobilenet.state_dict(),  # 모델 가중치 저장\n",
    "    'optimizer_state_dict': optimizer.state_dict(),  # 옵티마이저 상태 저장 (선택적)\n",
    "    'epoch': 10,  # 현재 학습 에폭 저장 (선택적) # 다른 필요한 정보 저장 (선택적)\n",
    "}, save_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracies['mnasnet']['top5']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "num_epochs = 5\n",
    "plt.title(\"Top1 Validation Accuracy\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),losses['mobilenet']['top1'],label=\"mobilenet\", color='purple')\n",
    "plt.plot(range(1,num_epochs+1),losses['mnasnet']['top1'],label=\"mnasnet\", color='red')\n",
    "plt.plot(range(1,num_epochs+1),losses['efficientnet']['top1'],label=\"efficientnet\", color='yellow')\n",
    "plt.plot(range(1,num_epochs+1),losses['shufflenet']['top1'],label=\"shufflenet\", color='green')\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "plt.title(\"Top5 Validation Accuracy\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),losses['mobilenet']['top5'],label=\"mobilenet\", color='purple')\n",
    "plt.plot(range(1,num_epochs+1),losses['mnasnet']['top5'],label=\"mnasnet\", color='red')\n",
    "plt.plot(range(1,num_epochs+1),losses['efficientnet']['top5'],label=\"efficientnet\", color='yellow')\n",
    "plt.plot(range(1,num_epochs+1),losses['shufflenet']['top5'],label=\"shufflenet\", color='green')\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "plt.title(\"Mobilenet Validation Accuracy: top1 VS top5\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),val_acc_history,label=\"top1 accuracy\", color='purple')\n",
    "plt.plot(range(1,num_epochs+1),val_top5_acc_history,label=\"top5 accuracy\", color='green')\n",
    "\n",
    "plt.ylim((0,1.))\n",
    "plt.yticks([0.8, 0.9, 1.0])\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 이미지 추론"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 모델 불러올 경로 설정\n",
    "# load_path = f'save/{model_name}_epoch{num_epochs}_quntize({quantize}.pth)'\n",
    "from PIL import Image\n",
    "\n",
    "# 이미지 추론 코드\n",
    "def infer(image_path, image_label):\n",
    "    since = time.time()\n",
    "    image = Image.open(image_path)\n",
    "    label = image_label\n",
    "    desired_size = (224, 224)\n",
    "    image = image.resize(desired_size)\n",
    "\n",
    "    # 모델 및 다른 정보 불러오기\n",
    "    load_path = 'save/mobilenet_epoch10_batch128_pretrained_noQuantize.pth'\n",
    "    checkpoint = torch.load(load_path)\n",
    "    model = CustomMobileNetV3Large(num_classes=500)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])  # 모델 가중치 불러오기\n",
    "    print(f'Model loaded from {load_path}')\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 사진을 모델 입력에 맞게 resize\n",
    "    preprocess  = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    input_tensor = preprocess(image).unsqueeze(0)\n",
    "    print('input tensor 사이즈 > ', input_tensor.size())\n",
    "    print(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor.to(device))\n",
    "\n",
    "    _, predicted_index = torch.max(outputs, 1)\n",
    "    predicted_index = predicted_index.item()\n",
    "\n",
    "    # 클래스 이름\n",
    "    with open('save/classes_batch128.pkl', 'rb') as pickle_file:\n",
    "        classes = pickle.load(pickle_file)\n",
    "        predicted_class = classes[predicted_index]\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # 모델이 예측한 클래스 인덱스 출력\n",
    "    print('='*20)\n",
    "    print('예측된 클래스 인덱스: ', predicted_index)\n",
    "    print('클래스 이름: ', predicted_class)\n",
    "    print('일치여부(클래스): ', predicted_class==label)\n",
    "    # print('일치여부(타겟): ', predicted_index==label)\n",
    "    print('추론 시간: ', end_time-since)\n",
    "    print('='*20)\n",
    "\n",
    "    return predicted_class, predicted_class==label\n",
    "\n",
    "# sample1: uncropped\n",
    "image1_path = 'save/K-039306(cropped).jpg'\n",
    "image1_label = 'K-039306'\n",
    "image1_predictiton, image1_correct = infer(image1_path, image1_label)\n",
    "\n",
    "# sample2: cropped\n",
    "image2_path = 'save/K-039306(uncropped).jpg'\n",
    "image2_label = 'K-039306'\n",
    "image2_predictiton, image2_correct = infer(image2_path, image2_label)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델들의 GFLOPS & Parameters 비교"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_size = (batch_size, 3, 224, 224)  # (배치 크기, 채널 수, 높이, 너비)\n",
    "\n",
    "# 모델 불러와 적용하기\n",
    "\n",
    "# model_names = ['mobilenet', 'mobilenet(quantized)', 'efficientnet', 'shufflenet']\n",
    "model_names = ['mobilenet', 'efficientnet', 'shufflenet']\n",
    "gflops_list = []\n",
    "params_list = []\n",
    "# mobilenet\n",
    "mobilenet = CustomMobileNetV3Large(num_classes=500)\n",
    "checkpoint = torch.load('save/CustomMobileNetV3Large_epoch5_quntize(False).pth')\n",
    "mobilenet.load_state_dict(checkpoint['model_state_dict'])  # 모델 가중치 불러오기\n",
    "optimizer = torch.optim.NAdam(mobilenet.parameters(), lr=0.001)\n",
    "mobilenet.eval()\n",
    "# 모델의 FLOPs 계산\n",
    "macs, params = profile(mobilenet, inputs=(torch.randn(*input_size),))\n",
    "gflops_list.append(round(macs / 1e9, 3))\n",
    "params_list.append(int(params/1000000))\n",
    "\n",
    "# mobilenet(quantized)\n",
    "# mobilenet_quantized = QuantizedCustomMobileNetV3Large(num_classes=500)\n",
    "# checkpoint = torch.load('save/CustomMobileNetV3Large_epoch5_quntize(False).pth')\n",
    "# mobilenet.load_state_dict(checkpoint['model_state_dict'])  # 모델 가중치 불러오기\n",
    "# optimizer = torch.optim.NAdam(mobilenet_quantized.parameters(), lr=0.001)\n",
    "# mobilenet.eval()\n",
    "# macs, params = profile(mobilenet_quantized, inputs=(torch.randn(*input_size),))\n",
    "# gflops_list.append(round(macs / 1e9, 3))\n",
    "# params_list.append(int(params/1000000))\n",
    "\n",
    "# 모델 불러와 적용하기\n",
    "efficientnet = models.efficientnet_b0(num_classes=500)\n",
    "checkpoint = torch.load('save/EfficientNet_epoch5_quntize(False).pth')\n",
    "efficientnet.load_state_dict(checkpoint['model_state_dict'])  # 모델 가중치 불러오기\n",
    "optimizer = torch.optim.NAdam(efficientnet.parameters(), lr=0.001)\n",
    "efficientnet.eval()\n",
    "macs, params = profile(efficientnet, inputs=(torch.randn(*input_size),))\n",
    "gflops_list.append(round(macs / 1e9, 3))\n",
    "params_list.append(int(params/1000000))\n",
    "\n",
    "# shufflenet\n",
    "shufflenet = models.shufflenet_v2_x2_0(num_classes=500)\n",
    "checkpoint = torch.load('save/ShuffleNetV2_epoch5_quntize(False).pth')\n",
    "shufflenet.load_state_dict(checkpoint['model_state_dict'])  # 모델 가중치 불러오기\n",
    "optimizer = torch.optim.NAdam(shufflenet.parameters(), lr=0.001)\n",
    "shufflenet.eval()\n",
    "macs, params = profile(shufflenet, inputs=(torch.randn(*input_size),))\n",
    "gflops_list.append(round(macs / 1e9, 3))\n",
    "params_list.append(int(params/1000000))\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Comparison of GFLOPs in Different Models')\n",
    "plt.xlabel('GFLOPs')\n",
    "plt.legend(loc='best')\n",
    "plt.bar(model_names, gflops_list, alpha=0.7, width=0.4, label='GFLOPs', color='b')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Comparison of number of Parameters(M) in Different Models')\n",
    "plt.xlabel('Parameters(M)')\n",
    "plt.legend(loc='best')\n",
    "plt.bar(model_names, params_list, alpha=0.7, width=0.4, label='Parameters(M)', color='g')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 테스트"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_model():\n",
    "  model = CustomMobileNetV3Large(num_classes=500)\n",
    "  checkpoint = torch.load('save/CustomMobileNetV3Large_epoch5_quntize(False).pth')\n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "  return model\n",
    "\n",
    "def test(test_loader):\n",
    "    model = get_model().eval()\n",
    "    correct = 0\n",
    "    top5_correct = 0\n",
    "    total = 0\n",
    "    predict_list = []\n",
    "    # 그라디언트 계산 비활성화 (테스트 모드)\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm_notebook(test_loader):\n",
    "            outputs = model(inputs)\n",
    "            _, top5 = torch.topk(outputs, k=5, dim=1)  # 상위 5개 예측을 가져옵니다.\n",
    "            total += labels.size(0)\n",
    "            # print(label, ' > ', predicted[:, 0] == labels)\n",
    "            top1 = top5[:, 0]\n",
    "            predict_list.append()\n",
    "            correct += (top5[:, 0] == labels).sum().item()  # 상위 1개 예측이 정답과 일치하는 경우\n",
    "            for i in range(5):\n",
    "                top5_correct += (top5[:, i] == labels).sum().item()  # 상위 5개 예측 중 하나가 정답과 일치하는 경우\n",
    "\n",
    "    accuracy = correct / total\n",
    "    top5_accuracy = top5_correct / total\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Top-5 Accuracy: {top5_accuracy * 100:.2f}%\")\n",
    "\n",
    "    return accuracy, top5_accuracy, predict_list\n",
    "\n",
    "test_loader =\n",
    "accuracy, top5_accuracy, predict_list = test(test_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## VIT 적용 및 학습"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "# 클래스 수\n",
    "num_classes = 500\n",
    "\n",
    "# 하이퍼파라미터\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 5\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []  # 이미지 파일 경로 리스트\n",
    "        self.labels = []  # 레이블 리스트\n",
    "\n",
    "        # 이미지 파일 경로와 레이블 로드\n",
    "        # 예: 이미지 파일 경로가 data_dir/class_name/image.jpg 형식일 때\n",
    "        for class_idx, class_name in enumerate(os.listdir(data_dir)):\n",
    "            class_path = os.path.join(data_dir, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                for image_name in os.listdir(class_path):\n",
    "                    image_path = os.path.join(class_path, image_name)\n",
    "                    self.image_paths.append(image_path)\n",
    "                    self.labels.append(class_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# 데이터 전처리 및 DataLoader 설정\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(data_dir=f'D:/data/training/sources/train', transform=data_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# ViT 모델 로드\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "feature_extractor = ViTFeatureExtractor(model_name)\n",
    "model = ViTForImageClassification.from_pretrained(model_name)\n",
    "classifier = nn.Linear(model.config.hidden_size, num_classes)\n",
    "model.classifier = classifier\n",
    "\n",
    "# 옵티마이저 및 손실 함수 설정\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 학습 루프\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in tqdm_notebook(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_path = f'save/vit_eph{num_epochs}.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': mobilenet.state_dict(),  # 모델 가중치 저장\n",
    "    'optimizer_state_dict': optimizer.state_dict(),  # 옵티마이저 상태 저장 (선택적)\n",
    "    'epoch': num_epochs,  # 현재 학습 에폭 저장 (선택적) # 다른 필요한 정보 저장 (선택적)\n",
    "}, save_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
