{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 필요한 모듈들 임포트"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine tuning\n",
    "사전 훈련된 모델을 기반으로 아키텍쳐를 새로운 목적에 맞게 변형하고 이미 학습된 모델의 가중치를 미세하게 조정해 학습시키는 방법\n",
    "    - 사전 훈련된 모델의 가중치를 초기값으로 사용하고 추가로 학습\n",
    "    - CNN 베이스의 사전학습 모델을 사용할 때에는 이전에 학습한 내용들을 모두 잊어버릴 위험이 있기 때문에 작은 learning rate를 사용하는것이 바람직함\n",
    "[방법1] 모델 전체를 새로 학습\n",
    "- 사전 학습 모델의 구조만 사용하면서, 자신의 데이터셋에 맞게 모델을 전부 새롭게 학습시키는 방법\n",
    "- 데이터의 크기가 크고 유사성이 작을 때 사용\n",
    "[방법2] Conv base는 일부분 고정(Freezing)하고 나머지 Conv Base 계층과 Classifier를 새롭게 학습\n",
    "- Conv base: 합성곱 층과 풀링 층이 여러 겹 쌓여있는 부분으로 특징을 추출하는 역할\n",
    "- Classifier: 주로 완전연결계층으로 구성되며 Conv base가 추출한 특징들을 잘 학습해 각각의 샘플들을 알맞은 class로 분류\n",
    "- 낮은 레벨의 계층은 일반적이고 독리적인 특징(신형성)을 추출하고, 높은 레벨의 계층은 구체적이고 명확한 특징(형상)을 추출\n",
    "- 크기가  크고 유사성도 높은 데이터셋일 때\n",
    "[방법3] Conv Base는 고정하고 Classifier만 새로 학습\n",
    "- 컴퓨팅 연산 능력이 부족하거나, 데이터셋이 너무 작을 때, 혹은 적용하려는 task와 사전학습에 쓰인 데이터가 매우 비슷할 때 사용\n",
    "\n",
    "[크기가 작고 유사성도 작은 데이터]\n",
    "- 방법2를 쓰되 조금 더 깊은 계층까지 새로 학습시키\n",
    "- Data Augmentation 하기\n",
    "\n",
    "[Feature Extraction]\n",
    "사전 훈련된 모델의 하위 층을 동결하고, 상위 층을 새로운 작업을 위해 수정하지 않고 사용하는 것 -> 데이터분류기(마지막 완전연결층) 부분만 새로 만드는 것\n",
    "    - 사전 훈련된 모델(고정): 중요한 특성 추출(학습 X)\n",
    "    - 데이터분류기: 추출된 특성을 입력받아 최종적으로 이미지에 대한 클래스를 분류(학습O)\n",
    "    - 가능한 모델: Xception, Inception V3, ResNet50, VGG16, VGG19, MobileNet\n",
    "\n",
    "[ 분류기 수정하는 방법 ]\n",
    "Fine tunning을 하기 전 input값으로 받는 이미지의 크기는 무조건 확인해야함\n",
    "    - 대부분의 모델은 (224, 224)이지만 inception_v3의 경우 (299, 299)\n",
    "1. print(model)로 마지막 층의 in_features 확인\n",
    "2. nn.Linear(in_features, num_classes)로 클래스 수 변경\n",
    "    - 예시1: Resnet\n",
    "        - (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
    "            - => model.fc = nn.Linear(512, num_classes)\n",
    "    - 예시2: Alexnet\n",
    "        - (classifier): Sequential(... (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
    "            - => model.classifier[6] = nn.Linear(4096, num_classes)\n",
    "    - 예시3(특이구조): Squeezenet\n",
    "        - - output은 classifier의 첫번째 레이어인 1X1 convolution layer로부터 나옴\n",
    "        - (classifier): Sequential((0): Dropout(p=0.5)  (1): Conv2d(512, 1000, kernel_size=13, stride=1, padding=0)  (2): ReLU(inplace)  (3): AvgPool2d(kernel_size=13, stride=1, padding=0))\n",
    "            -  => model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "    - 예시4(특이구조): Inception v3\n",
    "        - Rethinking을 하기 때문에 학습과정에서 output이 두개의 레이어로부터 나옴\n",
    "        - 두번째 output(auxiliary output): AuxLogits 부분을 포함하는 네트워크에 포함됨\n",
    "        - 주된 output은 네트워크의 마지막 레이어에서 출력됨\n",
    "            -  test 시에는 이 output만 고려함\n",
    "        - (AuxLogits): InceptionAux(... (fc): Linear(in_feature=768, out_features=1000, bias=True) ... (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
    "            -  finetune 하기 위해서는 두 레이어를 reshape 해줘야함\n",
    "            -  => model.AuxLogits.fc = nn.Linear(768, num_classes)\n",
    "            -  model.fc = nn.Linear(2048, nuum_classes)\n",
    "\n",
    "[ 사용가능한 함수들 ]\n",
    "- get_model(name, **config): 모델 이름과 환경설정을 인수로 받아 해당 모델의 인스턴스를 반환\n",
    "    - get_model(\"quantized_mobilenet_v3_large\", weights=\"DEFAULT\")\n",
    "- get_model_weight(name): 해당 모델의 열거형 가중치 클래스 반환\n",
    "    - 예1: get_model_weights(\"quantized_mobilenet_v3_large\")\n",
    "    - 예2: get_model_weights(torchvision.models.quantization.mobilenet_v3_large)\n",
    "    - 예1 = 예2\n",
    "- get_weight(name): 열거형 가중치 변수의 이름으로 값을 가져옴\n",
    "    - 예: get_weight(\"MobileNet_V3_Large_QuantizedWeights.DEFAULT\")\n",
    "- list_models(\\[module]): 해당 이름으로 등록된 모델 목록을 반환\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "import pandas\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from thop import profile\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "from torch.utils.data import random_split, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from custom_models import CustomMobileNetV3Large, QuantizedCustomMobileNetV3Large, CustomMnasNet1_3, QuantizedCustomMnasNet1_3\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "model_list = ['resnet', 'alexnet', 'vgg', 'squeezenet', 'densenet', 'inception', 'mobilenet']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 모델 파라미터의 requires_grad 속성\n",
    "- 기본적으로 사전  훈련된 모델을 로드할 때 모든 매개변수의 requires_grad = True로 설정되어 있음\n",
    "    - True: 처음부터 훈련하거나 fine tuning할 때는 True\n",
    "    - False(Freeze): 기존 layer의 weight를 고정"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 모델학습 헬퍼 함수\n",
    "def train_models(model, dataloaders, criterion, optimizer, num_epochs=5, is_inception=False, quantize=False):\n",
    "    torch.cuda.empty_cache()\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    val_top5_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    patience = 3\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1} / {num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # 각 에폭은 학습/검증 phase를 가짐\n",
    "        for phase in ['train', 'val']:\n",
    "            print(f'>>>>> phase: {phase}<<<<<')\n",
    "            if phase == 'train':\n",
    "                model.train()  # 트레이닝 모드 설정\n",
    "            else:\n",
    "                if quantize == True:\n",
    "                    # 모델을 양자화 + 추론모드\n",
    "                    model = torch.quantization.convert(model.eval(), inplace=False)\n",
    "                else:\n",
    "                    model.eval()  # 추론 모드 설정\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            running_top5_corrects = 0\n",
    "\n",
    "            # 데이터 학습하기\n",
    "            for inputs, labels in tqdm_notebook(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # 파라미터 기울기 초기화\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # 학습 모드인 경우에만 history 추적\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # 모델의 ouptut과 loss를 구함\n",
    "                    # inception의 경우 학습시 auxiliary output이 있는 특수 케이스임.\n",
    "                    #   학습시: final output과 auxiliary output을 더하는 과정이 필요함\n",
    "                    #   테스트시: final output만 고려\n",
    "                    # Auxilary output을 같이 고려해야하는 학습단계\n",
    "                    if is_inception and phase == 'train':\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4 * loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # top-5 정확도 계산\n",
    "                    _,top5_preds = torch.topk(outputs, 5)\n",
    "                    top5_corrects = torch.sum(top5_preds == labels.view(-1, 1))\n",
    "\n",
    "                    # 학습(backward + optimize)\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        if (epoch + 1) % 5 == 0:\n",
    "                            # 모델 및 다른 정보 저장\n",
    "                            save_path = f'save/{type(model).__name__}_epoch{epoch+1}_batchsize{batch_size}_quntize({quantize}).pth'\n",
    "                            torch.save({\n",
    "                                'model_state_dict': model.state_dict(),  # 모델 가중치 저장\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),  # 옵티마이저 상태 저장 (선택적)\n",
    "                                'epoch': epoch+1,  # 현재 학습 에폭 저장 (선택적)\n",
    "                                # 다른 필요한 정보 저장 (선택적)\n",
    "                            }, save_path)\n",
    "\n",
    "                # loss 구하기\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                running_top5_corrects += top5_corrects\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            epoch_top5_acc = running_top5_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # 모델 깊은복사\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                no_improvement_count = 0  # 개선이 있었으므로 카운트 초기화\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc.item())\n",
    "                val_top5_acc_history.append(epoch_top5_acc.item())\n",
    "                no_improvement_count += 1  # 개선이 없었으므로 카운트\n",
    "\n",
    "            # Early Stopping 확인\n",
    "            # if no_improvement_count >= patience:\n",
    "            #     print(f\"No improvement in validation accuracy for {patience} epochs. Early stopping...\")\n",
    "            #     break  # 학습 종료\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.4f}'.format(best_acc))\n",
    "    print('Best top5 val Acc: {:.4f}'.format(epoch_top5_acc))\n",
    "\n",
    "    # 베스트 모델 가중치를 로드\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history, val_top5_acc_history\n",
    "\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=True, quantize=False):\n",
    "    # 모델마다 다르게 지정될 변수들 초기화\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == 'resnet':\n",
    "        if quantize:\n",
    "            '''Quantized Resnet50'''\n",
    "            weights = models.quantization.ResNet50_QuantizedWeights.DEFAULT\n",
    "            model_ft = models.quantization.resnet50(weights=weights, quantize=True)\n",
    "        else:\n",
    "            '''Resnet50'''\n",
    "            model_ft = models.resnet50(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == 'mobilenet':\n",
    "        if quantize:\n",
    "            '''Quantized Mobilenet v3 large'''\n",
    "            model_ft = QuantizedCustomMobileNetV3Large(num_classes=500).to('cpu')\n",
    "\n",
    "            # 양자화 설정 - gbgemm 백엔드 사용\n",
    "            backend = 'fbgemm'\n",
    "            # 양자화 스키마 설정 (예: symmetric)\n",
    "            quantization_params = torch.quantization.get_default_qconfig(backend)\n",
    "            quantization_params = torch.quantization.QConfig(activation=quantization_params.activation, weight=quantization_params.weight)\n",
    "\n",
    "            # 양자화 준비\n",
    "            model_ft.qconfig = quantization_params\n",
    "            model_ft= torch.quantization.prepare_qat(model_ft, inplace=False)\n",
    "\n",
    "        else:\n",
    "            model_ft = CustomMobileNetV3Large(num_classes=500).to(device)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == 'mnasnet':\n",
    "        if quantize:\n",
    "            '''Quantized Mnasnet 1_3'''\n",
    "            model_ft = QuantizedCustomMnasNet1_3(num_classes=500).to(device)\n",
    "\n",
    "            # 양자화 설정 - gbgemm 백엔드 사용\n",
    "            backend = 'fbgemm'\n",
    "            model_ft.qconfig = torch.quantization.get_default_qat_qconfig(backend)\n",
    "\n",
    "            # 양자화 준비\n",
    "            model_ft= torch.quantization.prepare_qat(model_ft, inplace=False)\n",
    "\n",
    "        else:\n",
    "            model_ft = models.mnasnet1_3(weights=models.MNASNet1_3_Weights.IMAGENET1K_V1)\n",
    "            num_ftrs = model_ft.classifier[1].in_features\n",
    "            model_ft.classifier[1] = nn.Linear(in_features=1280, out_features=num_classes, bias=True)\n",
    "            # model_ft = CustomMnasNet1_3(num_classes=500).to(device)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == 'efficientnet':\n",
    "        if quantize:\n",
    "            pass\n",
    "        else:\n",
    "            model_ft = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "            num_ftrs = model_ft.classifier[1].in_features\n",
    "            model_ft.classifier[1] = nn.Linear(num_ftrs, num_classes, bias=True)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == 'shufflenet':\n",
    "        if quantize:\n",
    "            pass\n",
    "        else:\n",
    "            model_ft = models.shufflenet_v2_x2_0(weights=models.ShuffleNet_V2_X2_0_Weights.IMAGENET1K_V1)\n",
    "            num_ftrs = model_ft.fc.in_features\n",
    "            model_ft.fc = nn.Linear(num_ftrs, num_classes, bias=True)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == 'alexnet':\n",
    "        '''AlexNet'''\n",
    "        model_ft = models.alexnet(pretrianed=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_featrues\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == 'vgg':\n",
    "        '''VGG11_bn'''\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == 'squeezenet':\n",
    "        '''Squeezenet 1.0'''\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == 'densenet':\n",
    "        '''Densenet 121'''\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == 'inception':\n",
    "        '''Inception V3'''\n",
    "        if quantize:\n",
    "            weights = models.quantization.Inception_V3_QuantizedWeights.DEFAULT\n",
    "            model_ft = models.quantization.inception_v3(weights=weights, quantize=True)\n",
    "        else:\n",
    "            model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 299  # 다른 모델과 다르게 299 사이즈를 사용\n",
    "    else:\n",
    "        print('모델의 이름을 잘못 입력하여 종료합니다...')\n",
    "        exit()\n",
    "\n",
    "    return model_ft.to(device), input_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 데이터셋 생성1 - 증강없음"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 데이터 경로\n",
    "data_root = f'D:/data/training/sources/cropped'\n",
    "\n",
    "# 데이터 변환\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ImageFolder로 전체 데이터셋 생성\n",
    "dataset = datasets.ImageFolder(root=data_root, transform=data_transforms)\n",
    "\n",
    "# # 클래스 별 샘플 수 확인\n",
    "# class_counts = torch.bincount(torch.tensor(dataset.targets))\n",
    "# print('클래스별 샘플 수:', class_counts.unique())\n",
    "\n",
    "# 데이터셋 분할 (예: 80% 훈련, 20% 검증)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "# 데이터 로더 생성\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dataloaders_dict = {'train': train_dataloader, 'val': valid_dataloader}\n",
    "\n",
    "print('train > ', len(train_dataloader), 'val > ', len(valid_dataloader))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 데이터셋 셍성2 - 증강"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from shutil import copyfile\n",
    "from torchvision import datasets\n",
    "\n",
    "# 원본 데이터 폴더 경로\n",
    "data_root = 'D:/data/training/sources/cropped'  # 데이터 경로를 본인의 환경에 맞게 수정\n",
    "\n",
    "# 분할된 데이터를 저장할 폴더 경로\n",
    "train_root = 'D:/data/training/sources/train'\n",
    "valid_root = 'D:/data/training/sources/valid'\n",
    "\n",
    "# 클래스 이름을 가져오기 위한 함수\n",
    "def get_class_names(data_root):\n",
    "    class_names = os.listdir(data_root)\n",
    "    return class_names\n",
    "\n",
    "# 클래스 이름 목록 가져오기\n",
    "class_names = get_class_names(data_root)\n",
    "\n",
    "# 클래스별로 train과 valid 폴더 생성\n",
    "for class_name in class_names:\n",
    "    train_class_dir = os.path.join(train_root, class_name)\n",
    "    valid_class_dir = os.path.join(valid_root, class_name)\n",
    "\n",
    "    os.makedirs(train_class_dir, exist_ok=True)\n",
    "    os.makedirs(valid_class_dir, exist_ok=True)\n",
    "\n",
    "# 데이터를 8:2 비율로 나누어 train과 valid 폴더에 복사\n",
    "for class_name in class_names:\n",
    "    class_dir = os.path.join(data_root, class_name)\n",
    "    train_class_dir = os.path.join(train_root, class_name)\n",
    "    valid_class_dir = os.path.join(valid_root, class_name)\n",
    "\n",
    "    # 클래스 폴더 안의 모든 파일 목록 가져오기\n",
    "    files = os.listdir(class_dir)\n",
    "    num_files = len(files)\n",
    "    num_train_files = int(0.8 * num_files)\n",
    "\n",
    "    # 파일을 train과 valid 폴더에 복사\n",
    "    for i, file in enumerate(files):\n",
    "        src_path = os.path.join(class_dir, file)\n",
    "        if i < num_train_files:\n",
    "            dst_path = os.path.join(train_class_dir, file)\n",
    "        else:\n",
    "            dst_path = os.path.join(valid_class_dir, file)\n",
    "        copyfile(src_path, dst_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 데이터 변환: 데이터가 부족해 과적합이 발생한 것으로 보이므로 학습데이터 증강\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.GaussianBlur(kernel_size=(19, 19), sigma=(1.0, 2.0)),\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=0.1, p=0.5),\n",
    "    transforms.AutoAugment(policy=transforms.autoaugment.AutoAugmentPolicy.IMAGENET, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=(-30, 30)),  # 무작위 회전 (-30도에서 30도 사이)\n",
    "    transforms.RandomAffine(degrees=(-30, 30), translate=(0.1, 0.1), shear=0.1),\n",
    "    transforms.RandomPerspective(distortion_scale=0.5, p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "valid_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ImageFolder를 사용하여 train과 valid 데이터셋 생성\n",
    "train_dataset = datasets.ImageFolder(root=train_root, transform=train_transforms)\n",
    "valid_dataset = datasets.ImageFolder(root=valid_root, transform=valid_transforms)\n",
    "\n",
    "# 데이터 로더 생성\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "dataloaders_dict = {'train': train_dataloader, 'val': valid_dataloader}\n",
    "\n",
    "print('train > ', len(train_dataloader), 'val > ', len(valid_dataloader))\n",
    "\n",
    "# 학습\n",
    "\n",
    "mobilenet, input_size = initialize_model(model_name='mobilenet', num_classes=500, use_pretrained=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.NAdam(mobilenet.parameters(), lr=0.0004)\n",
    "\n",
    "mobilenet, val_acc_history, val_top5_acc_history = train_models(mobilenet, dataloaders_dict, criterion, optimizer, num_epochs=10, is_inception=False, quantize=False)\n",
    "\n",
    "accuracy_mobilenet = {\n",
    "    'top1': val_acc_history,\n",
    "    'top5': val_top5_acc_history}\n",
    "with open('save/accuracies_mobilenet_augmentated.json', 'w') as f:\n",
    "    json.dump(accuracy_mobilenet, f)\n",
    "\n",
    "save_path = f'save/mobilenet_epoch10_batch128_pretrained_Augmentated.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': mobilenet.state_dict(),  # 모델 가중치 저장\n",
    "    'optimizer_state_dict': optimizer.state_dict(),  # 옵티마이저 상태 저장 (선택적)\n",
    "    'epoch': 10,  # 현재 학습 에폭 저장 (선택적) # 다른 필요한 정보 저장 (선택적)\n",
    "}, save_path)\n",
    "\n",
    "num_epochs = 10\n",
    "plt.title(\"Mobilenet Validation Accuracy: top1 VS top5\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),val_acc_history,label=\"top1 accuracy\", color='purple')\n",
    "plt.plot(range(1,num_epochs+1),val_top5_acc_history,label=\"top5 accuracy\", color='green')\n",
    "\n",
    "plt.ylim((0,1.))\n",
    "plt.yticks([0.8, 0.9, 1.0])\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 데이터셋 생성3 - 증강 + 흑백"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision.transforms import Grayscale\n",
    "# 데이터 경로\n",
    "data_root = f'D:/data/training/sources/cropped'\n",
    "\n",
    "# ImageFolder로 전체 데이터셋 생성\n",
    "dataset = datasets.ImageFolder(root=data_root)\n",
    "\n",
    "# 데이터셋 분할 (예: 80% 훈련, 20% 검증)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "# 데이터 변환: 데이터가 모자라 과적합이 발생한 것으로 보이므로 학습데이터 증강\n",
    "train_transforms = transforms.Compose([\n",
    "    Grayscale(num_output_channels=1),  # 흑백 변환 (1 채널)\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=(-30, 30)),  # 무작위 회전 (-30도에서 30도 사이)\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), shear=0.1),\n",
    "    transforms.RandomPerspective(distortion_scale=0.5, p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485], [0.229])  # 흑백 이미지의 경우 평균과 표준편차는 하나의 값만 사용\n",
    "])\n",
    "\n",
    "valid_transforms = transforms.Compose([\n",
    "    Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485], [0.229])\n",
    "])\n",
    "\n",
    "# 클래스별 샘플 수 계산\n",
    "class_counts = torch.bincount(torch.tensor(train_dataset.dataset.targets))\n",
    "print('클래스별 샘플 수:', class_counts)\n",
    "\n",
    "# 클래스별 데이터 샘플 수를 기반으로 WeightedRandomSampler를 사용하여 샘플링\n",
    "# 클래스 가중치: 클래스별 역수로 계산 => 샘플 수가 적을수록 가중치가 높아짐\n",
    "class_weights = 1.0 / class_counts.double()\n",
    "# 클래스 가중치 적용\n",
    "sample_weights = class_weights[train_dataset.dataset.targets]\n",
    "# WeightedRandomSampler를 이용해 데이터 샘플링\n",
    "sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "\n",
    "# 데이터 로더 생성\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dataloaders_dict = {'train': train_dataloader, 'val': valid_dataloader}\n",
    "\n",
    "print('train > ', len(train_dataloader), 'val > ', len(valid_dataloader))\n",
    "# # 리스트를 텍스트 파일로 저장하기\n",
    "# with open(f'save/classes_batch{batch_size}.pkl', 'wb') as f:\n",
    "#     pickle.dump(dataset.classes, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train_classes = set()\n",
    "# for data in tqdm_notebook(train_dataset):\n",
    "#     _, label = data  # 데이터와 레이블을 분리\n",
    "#     train_classes.add(label)  # 레이블을 set에 추가하여 중복 클래스 제거\n",
    "#\n",
    "# num_train_classes = len(train_classes)\n",
    "# print(\"훈련 데이터셋 클래스 수:\", num_train_classes)\n",
    "#\n",
    "# valid_classes = set()\n",
    "# for data in tqdm_notebook(valid_dataset):\n",
    "#     _, label = data\n",
    "#     valid_classes.add(label)\n",
    "#\n",
    "# num_valid_classes = len(valid_classes)\n",
    "# print(\"검증 데이터셋 클래스 수:\", num_valid_classes)\n",
    "#\n",
    "# test_classes = set()\n",
    "# for data in tqdm_notebook(test_dataset):\n",
    "#     _, label = data\n",
    "#     test_classes.add(label)\n",
    "#\n",
    "# num_test_classes = len(test_classes)\n",
    "# print(\"테스트 데이터셋 클래스 수:\", num_test_classes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mobilenet, input_size = initialize_model(model_name='mobilenet', num_classes=500, use_pretrained=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.NAdam(mobilenet.parameters(), lr=0.0004)\n",
    "\n",
    "mobilenet, val_acc_history, val_top5_acc_history = train_models(mobilenet, dataloaders_dict, criterion, optimizer, num_epochs=10, is_inception=False, quantize=False)\n",
    "\n",
    "save_path = f'save/mobilenet_epoch10_batch128_pretrained_noQuantize.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': mobilenet.state_dict(),  # 모델 가중치 저장\n",
    "    'optimizer_state_dict': optimizer.state_dict(),  # 옵티마이저 상태 저장 (선택적)\n",
    "    'epoch': 10,  # 현재 학습 에폭 저장 (선택적) # 다른 필요한 정보 저장 (선택적)\n",
    "}, save_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def testing_models(models, dataloaders_dict, num_epochs, num_classes=1000, feature_extract=False, use_pretrained=True, quantize=False):\n",
    "    acc = {}\n",
    "\n",
    "    for model_name in models:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        model, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=use_pretrained, quantize=quantize)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.NAdam(model.parameters(), lr=0.001)\n",
    "\n",
    "        print('device > ', device)\n",
    "        print(f'{model_name} / pretrained({use_pretrained}) / quantize({quantize})')\n",
    "\n",
    "        model, hist, top5_hist = train_models(model, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs, is_inception=False, quantize=quantize)\n",
    "\n",
    "        acc[model_name] = {\n",
    "            'top1': hist,\n",
    "            'top5': top5_hist\n",
    "        }\n",
    "\n",
    "    return accuracies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = 'mobilenet'\n",
    "num_epochs = 2\n",
    "quantize = False\n",
    "model, hist, top5_hist = train_models(model_name, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs, is_inception=False, quantize=quantize)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# models = ['mobilenet', 'mnasnet', 'efficientnet', 'shufflenet']\n",
    "accuracies_quantized = testing_models(models=['mobilenet'], dataloaders_dict=dataloaders_dict, num_epochs=5, num_classes=500, feature_extract=False, use_pretrained=True, quantize=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracies= testing_models(models=['mobilenet'], dataloaders_dict=dataloaders_dict, num_epochs=5, num_classes=500, feature_extract=False, use_pretrained=True, quantize=False)\n",
    "print(accuracies)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('save/accuracies.json', 'w') as f:\n",
    "    json.dump(accuracies, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_path = f'save/mobilenet_epoch10.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': mobilenet.state_dict(),  # 모델 가중치 저장\n",
    "    'optimizer_state_dict': optimizer.state_dict(),  # 옵티마이저 상태 저장 (선택적)\n",
    "    'epoch': 10,  # 현재 학습 에폭 저장 (선택적) # 다른 필요한 정보 저장 (선택적)\n",
    "}, save_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracies['mnasnet']['top5']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "num_epochs = 5\n",
    "plt.title(\"Top1 Validation Accuracy\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),losses['mobilenet']['top1'],label=\"mobilenet\", color='purple')\n",
    "plt.plot(range(1,num_epochs+1),losses['mnasnet']['top1'],label=\"mnasnet\", color='red')\n",
    "plt.plot(range(1,num_epochs+1),losses['efficientnet']['top1'],label=\"efficientnet\", color='yellow')\n",
    "plt.plot(range(1,num_epochs+1),losses['shufflenet']['top1'],label=\"shufflenet\", color='green')\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "plt.title(\"Top5 Validation Accuracy\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),losses['mobilenet']['top5'],label=\"mobilenet\", color='purple')\n",
    "plt.plot(range(1,num_epochs+1),losses['mnasnet']['top5'],label=\"mnasnet\", color='red')\n",
    "plt.plot(range(1,num_epochs+1),losses['efficientnet']['top5'],label=\"efficientnet\", color='yellow')\n",
    "plt.plot(range(1,num_epochs+1),losses['shufflenet']['top5'],label=\"shufflenet\", color='green')\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "plt.title(\"Mobilenet Validation Accuracy: top1 VS top5\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),val_acc_history,label=\"top1 accuracy\", color='purple')\n",
    "plt.plot(range(1,num_epochs+1),val_top5_acc_history,label=\"top5 accuracy\", color='green')\n",
    "\n",
    "plt.ylim((0,1.))\n",
    "plt.yticks([0.8, 0.9, 1.0])\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 이미지 추론"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 모델 불러올 경로 설정\n",
    "# load_path = f'save/{model_name}_epoch{num_epochs}_quntize({quantize}.pth)'\n",
    "from PIL import Image\n",
    "\n",
    "# 이미지 추론 코드\n",
    "def infer(image_path, image_label):\n",
    "    since = time.time()\n",
    "    image = Image.open(image_path)\n",
    "    label = image_label\n",
    "    desired_size = (224, 224)\n",
    "    image = image.resize(desired_size)\n",
    "\n",
    "    # 모델 및 다른 정보 불러오기\n",
    "    load_path = 'save/mobilenet_epoch10_batch128_pretrained_noQuantize.pth'\n",
    "    checkpoint = torch.load(load_path)\n",
    "    model = CustomMobileNetV3Large(num_classes=500)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])  # 모델 가중치 불러오기\n",
    "    print(f'Model loaded from {load_path}')\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 사진을 모델 입력에 맞게 resize\n",
    "    preprocess  = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    input_tensor = preprocess(image).unsqueeze(0)\n",
    "    print('input tensor 사이즈 > ', input_tensor.size())\n",
    "    print(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor.to(device))\n",
    "\n",
    "    _, predicted_index = torch.max(outputs, 1)\n",
    "    predicted_index = predicted_index.item()\n",
    "\n",
    "    # 클래스 이름\n",
    "    with open('save/classes_batch128.pkl', 'rb') as pickle_file:\n",
    "        classes = pickle.load(pickle_file)\n",
    "        predicted_class = classes[predicted_index]\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # 모델이 예측한 클래스 인덱스 출력\n",
    "    print('='*20)\n",
    "    print('예측된 클래스 인덱스: ', predicted_index)\n",
    "    print('클래스 이름: ', predicted_class)\n",
    "    print('일치여부(클래스): ', predicted_class==label)\n",
    "    # print('일치여부(타겟): ', predicted_index==label)\n",
    "    print('추론 시간: ', end_time-since)\n",
    "    print('='*20)\n",
    "\n",
    "    return predicted_class, predicted_class==label\n",
    "\n",
    "# sample1: uncropped\n",
    "image1_path = 'save/K-039306(cropped).jpg'\n",
    "image1_label = 'K-039306'\n",
    "image1_predictiton, image1_correct = infer(image1_path, image1_label)\n",
    "\n",
    "# sample2: cropped\n",
    "image2_path = 'save/K-039306(uncropped).jpg'\n",
    "image2_label = 'K-039306'\n",
    "image2_predictiton, image2_correct = infer(image2_path, image2_label)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델들의 GFLOPS & Parameters 비교"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_size = (batch_size, 3, 224, 224)  # (배치 크기, 채널 수, 높이, 너비)\n",
    "\n",
    "# 모델 불러와 적용하기\n",
    "\n",
    "# model_names = ['mobilenet', 'mobilenet(quantized)', 'efficientnet', 'shufflenet']\n",
    "model_names = ['mobilenet', 'efficientnet', 'shufflenet']\n",
    "gflops_list = []\n",
    "params_list = []\n",
    "# mobilenet\n",
    "mobilenet = CustomMobileNetV3Large(num_classes=500)\n",
    "checkpoint = torch.load('save/CustomMobileNetV3Large_epoch5_quntize(False).pth')\n",
    "mobilenet.load_state_dict(checkpoint['model_state_dict'])  # 모델 가중치 불러오기\n",
    "optimizer = torch.optim.NAdam(mobilenet.parameters(), lr=0.001)\n",
    "mobilenet.eval()\n",
    "# 모델의 FLOPs 계산\n",
    "macs, params = profile(mobilenet, inputs=(torch.randn(*input_size),))\n",
    "gflops_list.append(round(macs / 1e9, 3))\n",
    "params_list.append(int(params/1000000))\n",
    "\n",
    "# mobilenet(quantized)\n",
    "# mobilenet_quantized = QuantizedCustomMobileNetV3Large(num_classes=500)\n",
    "# checkpoint = torch.load('save/CustomMobileNetV3Large_epoch5_quntize(False).pth')\n",
    "# mobilenet.load_state_dict(checkpoint['model_state_dict'])  # 모델 가중치 불러오기\n",
    "# optimizer = torch.optim.NAdam(mobilenet_quantized.parameters(), lr=0.001)\n",
    "# mobilenet.eval()\n",
    "# macs, params = profile(mobilenet_quantized, inputs=(torch.randn(*input_size),))\n",
    "# gflops_list.append(round(macs / 1e9, 3))\n",
    "# params_list.append(int(params/1000000))\n",
    "\n",
    "# 모델 불러와 적용하기\n",
    "efficientnet = models.efficientnet_b0(num_classes=500)\n",
    "checkpoint = torch.load('save/EfficientNet_epoch5_quntize(False).pth')\n",
    "efficientnet.load_state_dict(checkpoint['model_state_dict'])  # 모델 가중치 불러오기\n",
    "optimizer = torch.optim.NAdam(efficientnet.parameters(), lr=0.001)\n",
    "efficientnet.eval()\n",
    "macs, params = profile(efficientnet, inputs=(torch.randn(*input_size),))\n",
    "gflops_list.append(round(macs / 1e9, 3))\n",
    "params_list.append(int(params/1000000))\n",
    "\n",
    "# shufflenet\n",
    "shufflenet = models.shufflenet_v2_x2_0(num_classes=500)\n",
    "checkpoint = torch.load('save/ShuffleNetV2_epoch5_quntize(False).pth')\n",
    "shufflenet.load_state_dict(checkpoint['model_state_dict'])  # 모델 가중치 불러오기\n",
    "optimizer = torch.optim.NAdam(shufflenet.parameters(), lr=0.001)\n",
    "shufflenet.eval()\n",
    "macs, params = profile(shufflenet, inputs=(torch.randn(*input_size),))\n",
    "gflops_list.append(round(macs / 1e9, 3))\n",
    "params_list.append(int(params/1000000))\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Comparison of GFLOPs in Different Models')\n",
    "plt.xlabel('GFLOPs')\n",
    "plt.legend(loc='best')\n",
    "plt.bar(model_names, gflops_list, alpha=0.7, width=0.4, label='GFLOPs', color='b')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Comparison of number of Parameters(M) in Different Models')\n",
    "plt.xlabel('Parameters(M)')\n",
    "plt.legend(loc='best')\n",
    "plt.bar(model_names, params_list, alpha=0.7, width=0.4, label='Parameters(M)', color='g')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 테스트"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_model():\n",
    "  model = CustomMobileNetV3Large(num_classes=500)\n",
    "  checkpoint = torch.load('save/CustomMobileNetV3Large_epoch5_quntize(False).pth')\n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "  return model\n",
    "\n",
    "def test(test_dataloader):\n",
    "    model = get_model().eval()\n",
    "    correct = 0\n",
    "    top5_correct = 0\n",
    "    total = 0\n",
    "    predict_list = []\n",
    "    # 그라디언트 계산 비활성화 (테스트 모드)\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm_notebook(test_dataloader):\n",
    "            outputs = model(inputs)\n",
    "            _, top5 = torch.topk(outputs, k=5, dim=1)  # 상위 5개 예측을 가져옵니다.\n",
    "            total += labels.size(0)\n",
    "            # print(label, ' > ', predicted[:, 0] == labels)\n",
    "            top1 = top5[:, 0]\n",
    "            predict_list.append()\n",
    "            correct += (top5[:, 0] == labels).sum().item()  # 상위 1개 예측이 정답과 일치하는 경우\n",
    "            for i in range(5):\n",
    "                top5_correct += (top5[:, i] == labels).sum().item()  # 상위 5개 예측 중 하나가 정답과 일치하는 경우\n",
    "\n",
    "    accuracy = correct / total\n",
    "    top5_accuracy = top5_correct / total\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Top-5 Accuracy: {top5_accuracy * 100:.2f}%\")\n",
    "\n",
    "    return accuracy, top5_accuracy, predict_list\n",
    "\n",
    "accuracy, top5_accuracy, predict_list = test(test_dataloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## VIT 적용 및 학습"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "# 클래스 수\n",
    "num_classes = 500\n",
    "\n",
    "# 하이퍼파라미터\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 5\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []  # 이미지 파일 경로 리스트\n",
    "        self.labels = []  # 레이블 리스트\n",
    "\n",
    "        # 이미지 파일 경로와 레이블 로드\n",
    "        # 예: 이미지 파일 경로가 data_dir/class_name/image.jpg 형식일 때\n",
    "        for class_idx, class_name in enumerate(os.listdir(data_dir)):\n",
    "            class_path = os.path.join(data_dir, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                for image_name in os.listdir(class_path):\n",
    "                    image_path = os.path.join(class_path, image_name)\n",
    "                    self.image_paths.append(image_path)\n",
    "                    self.labels.append(class_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# 데이터 전처리 및 DataLoader 설정\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(data_dir=f'D:/data/training/sources/cropped', transform=data_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# ViT 모델 로드\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "feature_extractor = ViTFeatureExtractor(model_name)\n",
    "model = ViTForImageClassification.from_pretrained(model_name)\n",
    "classifier = nn.Linear(model.config.hidden_size, num_classes)\n",
    "model.classifier = classifier\n",
    "\n",
    "# 옵티마이저 및 손실 함수 설정\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 학습 루프\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in tqdm_notebook(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_path = f'save/vit_eph{num_epochs}.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': mobilenet.state_dict(),  # 모델 가중치 저장\n",
    "    'optimizer_state_dict': optimizer.state_dict(),  # 옵티마이저 상태 저장 (선택적)\n",
    "    'epoch': num_epochs,  # 현재 학습 에폭 저장 (선택적) # 다른 필요한 정보 저장 (선택적)\n",
    "}, save_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 추론\n",
    "model.eval()\n",
    "\n",
    "# 추론할 이미지 전처리\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = data_transform(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    return image.to(device)\n",
    "\n",
    "# 추론할 이미지 파일 경로\n",
    "input_image_path = 'save/K-039306(cropped).jpg'\n",
    "\n",
    "# 이미지 추론\n",
    "with torch.no_grad():\n",
    "    input_image = preprocess_image(input_image_path)\n",
    "    outputs = model(input_image)\n",
    "    _, predicted = torch.max(outputs.logits, 1)\n",
    "\n",
    "# 예측 결과 출력\n",
    "print(f\"Predicted class: {predicted.item()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
